{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1> Structured data prediction using Cloud AI Platform </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Create a BigQuery Dataset and Google Cloud Storage Bucket \n",
    "<li> Export from BigQuery to CSVs in GCS\n",
    "<li> Training on Cloud AI Platform\n",
    "<li> Deploy trained model\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nny3m465gKkY"
   },
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery==3.4.1\n",
      "  Downloading google_cloud_bigquery-3.4.1-py2.py3-none-any.whl (215 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.1/215.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0dev,>=1.47.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==3.4.1) (1.53.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==3.4.1) (2.10.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==3.4.1) (2.28.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==3.4.1) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==3.4.1) (2.4.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==3.4.1) (1.22.2)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.22.3-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery==3.4.1) (2.8.2)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery==3.4.1) (2.17.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery==3.4.1) (1.59.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery==3.4.1) (1.48.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery==3.4.1) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-bigquery==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery==3.4.1) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery==3.4.1) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery==3.4.1) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery==3.4.1) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery==3.4.1) (3.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery==3.4.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery==3.4.1) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery==3.4.1) (5.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery==3.4.1) (0.4.8)\n",
      "Installing collected packages: protobuf, packaging, google-cloud-bigquery\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.4 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorboardx 2.6 requires protobuf<4,>=3.8.0, but you have protobuf 4.22.3 which is incompatible.\n",
      "keras-tuner 1.3.4 requires protobuf<=3.20.3, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-vision 3.4.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-videointelligence 1.16.3 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-spanner 3.30.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-resource-manager 1.9.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-pubsub 2.16.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-language 1.3.2 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-dlp 3.12.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-artifact-registry 1.8.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "apache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-bigquery-3.4.1 packaging-21.3 protobuf-4.22.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --user google-cloud-bigquery==3.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Restart your kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kindly ignore the deprecation warnings and incompatibility errors related to google-cloud-storage**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJ7ByvoXzpVI"
   },
   "source": [
    "## Set up environment variables and load necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set environment variables so that we can use them throughout the entire notebook. We will be using our project name for our bucket, so you only need to change your project and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'cloud-hero-fix' # Replace with the your bucket name\n",
    "PROJECT = 'pod-fr-retail' # Replace with your project-id\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.3\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is: pod-fr-retail\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"$PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L0-vOB4y2BJM"
   },
   "source": [
    "## The source dataset\n",
    "\n",
    "Our dataset is hosted in [BigQuery](https://cloud.google.com/bigquery/). The CDC's Natality data has details on US births from 1969 to 2008 and is a publically available dataset, meaning anyone with a GCP account has access. Click [here](https://console.cloud.google.com/bigquery?project=bigquery-public-data&p=publicdata&d=samples&t=natality&page=table) to access the dataset.\n",
    "\n",
    "The natality dataset is relatively large at almost 138 million rows and 31 columns, but simple to understand. `weight_pounds` is the target, the continuous value we’ll train a model to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BigQuery Dataset and Google Cloud Storage Bucket \n",
    "\n",
    "A BigQuery dataset is a container for tables, views, and models built with BigQuery ML. Let's create one called __babyweight__. We'll do the same for a GCS bucket for our project too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BigQuery dataset titled: babyweight\n",
      "Dataset 'pod-fr-retail:babyweight' successfully created.\n",
      "Here are your current datasets:\n",
      "                              datasetId                               \n",
      " -------------------------------------------------------------------- \n",
      "  DA_Analysis                                                         \n",
      "  admin_data                                                          \n",
      "  airbnb_sf                                                           \n",
      "  apiData                                                             \n",
      "  assets_iam                                                          \n",
      "  automation                                                          \n",
      "  babyweight                                                          \n",
      "  bank_data                                                           \n",
      "  bigquery_analysis                                                   \n",
      "  bqml_small                                                          \n",
      "  css_retail                                                          \n",
      "  dataform                                                            \n",
      "  demo                                                                \n",
      "  demoEcommerce                                                       \n",
      "  demoUS                                                              \n",
      "  demo_BQOpti                                                         \n",
      "  demo_ml_models                                                      \n",
      "  demo_supply_eu_test                                                 \n",
      "  demobestbuy                                                         \n",
      "  discoveryCatalog                                                    \n",
      "  dwh_logs                                                            \n",
      "  external_data                                                       \n",
      "  fd_temp                                                             \n",
      "  googleearth                                                         \n",
      "  gr_shared                                                           \n",
      "  int_part                                                            \n",
      "  jmugicag_mlcoaching                                                 \n",
      "  jmugicag_mlcoaching_us                                              \n",
      "  k_temp                                                              \n",
      "  k_temp_gcp_sa_retail                                                \n",
      "  kaggle                                                              \n",
      "  master_demo                                                         \n",
      "  model_deployment_monitoring_6245450346139746304                     \n",
      "  orange                                                              \n",
      "  prediction_products_catalog_2021210211533_2021_02_11T05_34_11_281Z  \n",
      "  product_vision_search                                               \n",
      "  retail_ai                                                           \n",
      "  supply_demo                                                         \n",
      "  td2bq                                                               \n",
      "  test                                                                \n",
      "  test2                                                               \n",
      "  workspace_demo                                                      \n",
      "Bucket exists, let's not recreate it.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Create a BigQuery dataset for babyweight if it doesn't exist\n",
    "datasetexists=$(bq ls -d | grep -w babyweight)\n",
    "\n",
    "if [ -n \"$datasetexists\" ]; then\n",
    "    echo -e \"BigQuery dataset already exists, let's not recreate it.\"\n",
    "\n",
    "else\n",
    "    echo \"Creating BigQuery dataset titled: babyweight\"\n",
    "    \n",
    "    bq --location=US mk --dataset \\\n",
    "        --description \"Babyweight\" \\\n",
    "        $PROJECT:babyweight\n",
    "    echo \"Here are your current datasets:\"\n",
    "    bq ls\n",
    "fi\n",
    "    \n",
    "## Create GCS bucket if it doesn't exist already...\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "\n",
    "if [ -n \"$exists\" ]; then\n",
    "    echo -e \"Bucket exists, let's not recreate it.\"\n",
    "    \n",
    "else\n",
    "    echo \"Creating a new GCS bucket.\"\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "    echo \"Here are your current buckets:\"\n",
    "    gsutil ls\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b2TuS1s9vREL"
   },
   "source": [
    "## Create the training and evaluation data tables\n",
    "\n",
    "Since there is already a publicly available dataset, we can simply create the training and evaluation data tables using this raw input data. First we are going to create a subset of the data limiting our columns to `weight_pounds`, `is_male`, `mother_age`, `plurality`, and `gestation_weeks` as well as some simple filtering and a column to hash on for repeatable splitting.\n",
    "\n",
    "* Note:  The dataset in the create table code below is the one created previously, e.g. \"babyweight\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and filter dataset\n",
    "\n",
    "We have some preprocessing and filtering we would like to do to get our data in the right format for training.\n",
    "\n",
    "Preprocessing:\n",
    "* Cast `is_male` from `BOOL` to `STRING`\n",
    "* Cast `plurality` from `INTEGER` to `STRING` where `[1, 2, 3, 4, 5]` becomes `[\"Single(1)\", \"Twins(2)\", \"Triplets(3)\", \"Quadruplets(4)\", \"Quintuplets(5)\"]`\n",
    "* Add `hashcolumn` hashing on `year` and `month`\n",
    "\n",
    "Filtering:\n",
    "* Only want data for years later than `2000`\n",
    "* Only want baby weights greater than `0`\n",
    "* Only want mothers whose age is greater than `0`\n",
    "* Only want plurality to be greater than `0`\n",
    "* Only want the number of weeks of gestation to be greater than `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b0f0a5fec44a3eb6678ed7ba18c51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_data AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    CAST(is_male AS STRING) AS is_male,\n",
    "    mother_age,\n",
    "    CASE\n",
    "        WHEN plurality = 1 THEN \"Single(1)\"\n",
    "        WHEN plurality = 2 THEN \"Twins(2)\"\n",
    "        WHEN plurality = 3 THEN \"Triplets(3)\"\n",
    "        WHEN plurality = 4 THEN \"Quadruplets(4)\"\n",
    "        WHEN plurality = 5 THEN \"Quintuplets(5)\"\n",
    "    END AS plurality,\n",
    "    gestation_weeks,\n",
    "    FARM_FINGERPRINT(\n",
    "        CONCAT(\n",
    "            CAST(year AS STRING),\n",
    "            CAST(month AS STRING)\n",
    "        )\n",
    "    ) AS hashmonth\n",
    "FROM\n",
    "    publicdata.samples.natality\n",
    "WHERE\n",
    "    year > 2000\n",
    "    AND weight_pounds > 0\n",
    "    AND mother_age > 0\n",
    "    AND plurality > 0\n",
    "    AND gestation_weeks > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment dataset to simulate missing data\n",
    "\n",
    "Now we want to augment our dataset with our simulated babyweight data by setting all gender information to `Unknown` and setting plurality of all non-single births to `Multiple(2+)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80cbf88df0284689839cf9382f043776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_augmented_data AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks,\n",
    "    hashmonth\n",
    "FROM\n",
    "    babyweight.babyweight_data\n",
    "UNION ALL\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    \"Unknown\" AS is_male,\n",
    "    mother_age,\n",
    "    CASE\n",
    "        WHEN plurality = \"Single(1)\" THEN plurality\n",
    "        ELSE \"Multiple(2+)\"\n",
    "    END AS plurality,\n",
    "    gestation_weeks,\n",
    "    hashmonth\n",
    "FROM\n",
    "    babyweight.babyweight_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split augmented dataset into train and eval sets\n",
    "\n",
    "Using `hashmonth`, apply a module to get approximately a 75/25 train-eval split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split augmented dataset into train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMNRractvREL"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4468bcb262e34511b6c22e897a6ca01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_data_train AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks\n",
    "FROM\n",
    "    babyweight.babyweight_augmented_data\n",
    "WHERE\n",
    "    ABS(MOD(hashmonth, 4)) < 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split augmented dataset into eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b6061f3d99410e872546244fce153b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    babyweight.babyweight_data_eval AS\n",
    "SELECT\n",
    "    weight_pounds,\n",
    "    is_male,\n",
    "    mother_age,\n",
    "    plurality,\n",
    "    gestation_weeks\n",
    "FROM\n",
    "    babyweight.babyweight_augmented_data\n",
    "WHERE\n",
    "    ABS(MOD(hashmonth, 4)) = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clnaaqQsXkwC"
   },
   "source": [
    "## Verify table creation\n",
    "\n",
    "Verify that you created the dataset and training data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f69f4b88864879b64eea933c5f5c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc6a38492434cf9955d292862181a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: |          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_pounds</th>\n",
       "      <th>is_male</th>\n",
       "      <th>mother_age</th>\n",
       "      <th>plurality</th>\n",
       "      <th>gestation_weeks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [weight_pounds, is_male, mother_age, plurality, gestation_weeks]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "-- LIMIT 0 is a free query; this allows us to check that the table exists.\n",
    "SELECT * FROM babyweight.babyweight_data_train\n",
    "LIMIT 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251ff0678190473d9dba3f9834ec9ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030d9a9255c34183a8a0c4430b2ee90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: |          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight_pounds</th>\n",
       "      <th>is_male</th>\n",
       "      <th>mother_age</th>\n",
       "      <th>plurality</th>\n",
       "      <th>gestation_weeks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [weight_pounds, is_male, mother_age, plurality, gestation_weeks]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "-- LIMIT 0 is a free query; this allows us to check that the table exists.\n",
    "SELECT * FROM babyweight.babyweight_data_eval\n",
    "LIMIT 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export from BigQuery to CSVs in GCS\n",
    "\n",
    "Use BigQuery Python API to export our train and eval tables to Google Cloud Storage in the CSV format to be used later for TensorFlow/Keras training. We'll want to use the dataset we've been using above as well as repeat the process for both training and evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported pod-fr-retail:babyweight.babyweight_data_train to gs://cloud-hero-fix/babyweight/data/train*.csv\n",
      "Exported pod-fr-retail:babyweight.babyweight_data_eval to gs://cloud-hero-fix/babyweight/data/eval*.csv\n"
     ]
    }
   ],
   "source": [
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "dataset_name = \"babyweight\"\n",
    "\n",
    "# Create dataset reference object\n",
    "dataset_ref = client.dataset(\n",
    "    dataset_id=dataset_name, project=client.project)\n",
    "\n",
    "# Export both train and eval tables\n",
    "for step in [\"train\", \"eval\"]:\n",
    "    destination_uri = os.path.join(\n",
    "        \"gs://\", BUCKET, dataset_name, \"data\", \"{}*.csv\".format(step))\n",
    "    table_name = \"babyweight_data_{}\".format(step)\n",
    "    table_ref = dataset_ref.table(table_name)\n",
    "    extract_job = client.extract_table(\n",
    "        table_ref,\n",
    "        destination_uri,\n",
    "        # Location must match that of the source table.\n",
    "        location=\"US\",\n",
    "    )  # API request\n",
    "    extract_job.result()  # Waits for job to complete.\n",
    "\n",
    "    print(\"Exported {}:{}.{} to {}\".format(\n",
    "        client.project, dataset_name, table_name, destination_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify CSV creation\n",
    "\n",
    "Verify that we correctly created the CSV files in our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-hero-fix/babyweight/data/eval000000000000.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000001.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000002.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000003.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000004.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000005.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000006.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000007.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000008.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000009.csv\n",
      "gs://cloud-hero-fix/babyweight/data/eval000000000010.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000000.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000001.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000002.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000003.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000004.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000005.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000006.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000007.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000008.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000009.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000010.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000011.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000012.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000013.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000014.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000015.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000016.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000017.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000018.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000019.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000020.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000021.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000022.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000023.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000024.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000025.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000026.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000027.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000028.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000029.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000030.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000031.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000032.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000033.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000034.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000035.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000036.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000037.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000038.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000039.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000040.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000041.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000042.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data exists\n",
    "\n",
    "Verify that you previously created CSV files we'll be using for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-hero-fix/babyweight/data/eval000000000000.csv\n",
      "gs://cloud-hero-fix/babyweight/data/train000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/data/*000000000000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training on Vertex AI\n",
    "\n",
    "Now that we see everything is working locally, it's time to train on the cloud! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit to the Cloud we use [`gcloud ai custom-jobs create`](https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create) and simply specify some additional parameters.\n",
    "\n",
    "Below the `-- \\` we add in the arguments for our `task.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud artifacts repositories create babyweight \\\n",
    "    --repository-format=Docker \\\n",
    "    --location=${REGION} \\\n",
    "    --description=\"repository for babyweight model images\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_URI = f\"{REGION}-docker.pkg.dev/{PROJECT}/babyweight/babyweight-image\"\n",
    "os.environ[\"IMAGE_URI\"] = IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-central1-docker.pkg.dev/pod-fr-retail/babyweight/babyweight-image'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2023-04-19T09:47:01.829964Z'\n",
      "description: repository for babyweight model images\n",
      "dockerConfig: {}\n",
      "format: DOCKER\n",
      "mode: STANDARD_REPOSITORY\n",
      "name: projects/pod-fr-retail/locations/us-central1/repositories/babyweight\n",
      "updateTime: '2023-04-19T09:47:01.829964Z'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encryption: Google-managed key\n",
      "Repository Size: 1406.967MB\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud artifacts repositories describe babyweight \\\n",
    "    --location=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"europe-docker.pkg.dev\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud auth configure-docker ${REGION}-docker.pkg.dev --project=${PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   32.3kB\n",
      "Step 1/8 : FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest\n",
      " ---> 9936af9b8ff5\n",
      "Step 2/8 : RUN mkdir -m 777 -p /usr/app /home\n",
      " ---> Running in db6ea6e2260d\n",
      "Removing intermediate container db6ea6e2260d\n",
      " ---> f94859faecc9\n",
      "Step 3/8 : WORKDIR /usr/app\n",
      " ---> Running in 722c3b11dfdc\n",
      "Removing intermediate container 722c3b11dfdc\n",
      " ---> 9aa34d72b078\n",
      "Step 4/8 : ENV HOME=/home\n",
      " ---> Running in a3295ffc0648\n",
      "Removing intermediate container a3295ffc0648\n",
      " ---> a13ecf415b26\n",
      "Step 5/8 : ENV PYTHONDONTWRITEBYTECODE=1\n",
      " ---> Running in 401cd34b9088\n",
      "Removing intermediate container 401cd34b9088\n",
      " ---> fe890a417d73\n",
      "Step 6/8 : RUN rm -rf /var/sitecustomize\n",
      " ---> Running in 4e8b21265d01\n",
      "Removing intermediate container 4e8b21265d01\n",
      " ---> 5f798ab1c60b\n",
      "Step 7/8 : COPY [\".\", \".\"]\n",
      " ---> 781b905f7581\n",
      "Step 8/8 : ENTRYPOINT [\"python3\", \"task.py\"]\n",
      " ---> Running in 01f1563b2ead\n",
      "Removing intermediate container 01f1563b2ead\n",
      " ---> 742d5497a913\n",
      "Successfully built 742d5497a913\n",
      "Successfully tagged us-central1-docker.pkg.dev/pod-fr-retail/babyweight/babyweight-image:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/pod-fr-retail/babyweight/babyweight-image]\n",
      "fb6eda3af909: Preparing\n",
      "00097fcea6b4: Preparing\n",
      "819f2baabd34: Preparing\n",
      "e42695c7b436: Preparing\n",
      "e42695c7b436: Preparing\n",
      "72f0f663075e: Preparing\n",
      "80e7b44838bc: Preparing\n",
      "d00d79582461: Preparing\n",
      "bb390c9a7321: Preparing\n",
      "bb390c9a7321: Preparing\n",
      "2c6166174dc3: Preparing\n",
      "195dd9e4596e: Preparing\n",
      "8ea2e867cb61: Preparing\n",
      "d814e596ef88: Preparing\n",
      "3af8a8a4d9a1: Preparing\n",
      "556b3e96b91a: Preparing\n",
      "a6c56e90f067: Preparing\n",
      "a6c56e90f067: Preparing\n",
      "d9148730a7c0: Preparing\n",
      "e5dfab58fb0d: Preparing\n",
      "e5a278de147d: Preparing\n",
      "f61c41b5e0d9: Preparing\n",
      "84ebe34d5fc9: Preparing\n",
      "5d1ba21bd436: Preparing\n",
      "dcf153d49379: Preparing\n",
      "5f4c5f7f9a88: Preparing\n",
      "d4c7adc674b9: Preparing\n",
      "25b672beea00: Preparing\n",
      "c381da37ef45: Preparing\n",
      "bbf4bedf16ab: Preparing\n",
      "b9ed9dc50710: Preparing\n",
      "1d664bb87d46: Preparing\n",
      "1c73d996bda1: Preparing\n",
      "a472ae45df62: Preparing\n",
      "efc1998725ed: Preparing\n",
      "263b3cc9fb4c: Preparing\n",
      "2e8ff0fc8729: Preparing\n",
      "7728ec2832ab: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "998dc2d46e33: Preparing\n",
      "89d0d239dcf7: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "1e6620f29653: Preparing\n",
      "d60a43dc20a0: Preparing\n",
      "6021993d84a2: Preparing\n",
      "d4c7adc674b9: Waiting\n",
      "25b672beea00: Waiting\n",
      "c381da37ef45: Waiting\n",
      "bbf4bedf16ab: Waiting\n",
      "b9ed9dc50710: Waiting\n",
      "1d664bb87d46: Waiting\n",
      "1c73d996bda1: Waiting\n",
      "a472ae45df62: Waiting\n",
      "efc1998725ed: Waiting\n",
      "263b3cc9fb4c: Waiting\n",
      "2e8ff0fc8729: Waiting\n",
      "80e7b44838bc: Waiting\n",
      "7728ec2832ab: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "d00d79582461: Waiting\n",
      "998dc2d46e33: Waiting\n",
      "bb390c9a7321: Waiting\n",
      "89d0d239dcf7: Waiting\n",
      "2c6166174dc3: Waiting\n",
      "1e6620f29653: Waiting\n",
      "195dd9e4596e: Waiting\n",
      "d60a43dc20a0: Waiting\n",
      "8ea2e867cb61: Waiting\n",
      "6021993d84a2: Waiting\n",
      "d814e596ef88: Waiting\n",
      "f61c41b5e0d9: Waiting\n",
      "3af8a8a4d9a1: Waiting\n",
      "556b3e96b91a: Waiting\n",
      "84ebe34d5fc9: Waiting\n",
      "5d1ba21bd436: Waiting\n",
      "a6c56e90f067: Waiting\n",
      "dcf153d49379: Waiting\n",
      "d9148730a7c0: Waiting\n",
      "5f4c5f7f9a88: Waiting\n",
      "e5dfab58fb0d: Waiting\n",
      "e5a278de147d: Waiting\n",
      "e42695c7b436: Layer already exists\n",
      "72f0f663075e: Layer already exists\n",
      "d00d79582461: Layer already exists\n",
      "bb390c9a7321: Layer already exists\n",
      "2c6166174dc3: Layer already exists\n",
      "80e7b44838bc: Layer already exists\n",
      "8ea2e867cb61: Layer already exists\n",
      "195dd9e4596e: Layer already exists\n",
      "d814e596ef88: Layer already exists\n",
      "556b3e96b91a: Layer already exists\n",
      "a6c56e90f067: Layer already exists\n",
      "3af8a8a4d9a1: Layer already exists\n",
      "d9148730a7c0: Layer already exists\n",
      "00097fcea6b4: Pushed\n",
      "f61c41b5e0d9: Layer already exists\n",
      "e5dfab58fb0d: Layer already exists\n",
      "fb6eda3af909: Pushed\n",
      "84ebe34d5fc9: Layer already exists\n",
      "e5a278de147d: Layer already exists\n",
      "dcf153d49379: Layer already exists\n",
      "d4c7adc674b9: Layer already exists\n",
      "25b672beea00: Layer already exists\n",
      "c381da37ef45: Layer already exists\n",
      "819f2baabd34: Pushed\n",
      "1d664bb87d46: Layer already exists\n",
      "5f4c5f7f9a88: Layer already exists\n",
      "1c73d996bda1: Layer already exists\n",
      "efc1998725ed: Layer already exists\n",
      "bbf4bedf16ab: Layer already exists\n",
      "b9ed9dc50710: Layer already exists\n",
      "263b3cc9fb4c: Layer already exists\n",
      "7728ec2832ab: Layer already exists\n",
      "a472ae45df62: Layer already exists\n",
      "89d0d239dcf7: Layer already exists\n",
      "2e8ff0fc8729: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "998dc2d46e33: Layer already exists\n",
      "6021993d84a2: Layer already exists\n",
      "1e6620f29653: Layer already exists\n",
      "d60a43dc20a0: Layer already exists\n",
      "5d1ba21bd436: Pushed\n",
      "latest: digest: sha256:7e837765002d2561cf2a0c495212fbea1dcfc999564c3c38c98afbdfdb1b1d67 size: 9716\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:935: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "\n",
      "A custom container image is built locally.\n",
      "\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:935: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "\n",
      "Custom container image [us-central1-docker.pkg.dev/pod-fr-retail/babyweight/babyweight-image] is created for your custom job.\n",
      "\n",
      "CustomJob [projects/486742359899/locations/us-central1/customJobs/6163297448652439552] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/486742359899/locations/us-central1/customJobs/6163297448652439552\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/486742359899/locations/us-central1/customJobs/6163297448652439552\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBID=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "OUTPUT_IMAGE_URI=${IMAGE_URI}\n",
    "\n",
    "gcloud ai custom-jobs create \\\n",
    "    --display-name=${JOBID} \\\n",
    "    --region=${REGION} \\\n",
    "    --worker-pool-spec=replica-count=1,machine-type='n1-standard-8',executor-image-uri=us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11:latest,local-package-path=$(pwd)/babyweight/trainer,script=task.py,output-image-uri=${OUTPUT_IMAGE_URI} \\\n",
    "    --args=--train_data_path=gs://${BUCKET}/babyweight/data/train*.csv,--eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv,--output_dir=${OUTDIR},--num_epochs=10,--train_examples=10000,--eval_steps=100,--batch_size=32,--nembeds=8\n",
    "echo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job should complete within 15 to 20 minutes. To see the progress, in your Cloud Console, click on Navigation menu > AI Platform > Jobs. You do not need to wait for this training job to finish before moving forward in the notebook, but will need a trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check our trained model files\n",
    "\n",
    "Let's check the directory structure of our outputs of our trained model in folder we exported. We'll want to deploy the saved_model.pb within the timestamped directory as well as the variable values in the variables folder. Therefore, we need the path of the timestamped directory so that everything within it can be found by Cloud AI Platform's model deployment service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-hero-fix/babyweight/trained_model/\n",
      "gs://cloud-hero-fix/babyweight/trained_model/20230419090528/\n",
      "gs://cloud-hero-fix/babyweight/trained_model/checkpoints/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://cloud-hero-fix/babyweight/trained_model/20230419090528/\n",
      "gs://cloud-hero-fix/babyweight/trained_model/20230419090528/fingerprint.pb\n",
      "gs://cloud-hero-fix/babyweight/trained_model/20230419090528/saved_model.pb\n",
      "gs://cloud-hero-fix/babyweight/trained_model/20230419090528/assets/\n",
      "gs://cloud-hero-fix/babyweight/trained_model/20230419090528/variables/\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(gsutil ls -ld -- gs://${BUCKET}/babyweight/trained_model/2* \\\n",
    "                 | tail -1)\n",
    "gsutil ls ${MODEL_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy trained model\n",
    "\n",
    "Deploying the trained model to act as a REST web service is a simple gcloud call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_NAME=\"babyweight_endpoint\"\n",
    "MODEL_NAME=\"babyweight\"\n",
    "MODEL_ID=\"babyweight_\"+time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "ENDPOINT_ID=\"babyweight\"+time.strftime(\"%Y%m%d-%H%M%S\")+\"a\"\n",
    "os.environ[\"ENDPOINT_NAME\"] = ENDPOINT_NAME\n",
    "os.environ[\"MODEL_NAME\"] = MODEL_NAME\n",
    "os.environ[\"MODEL_ID\"] = MODEL_ID\n",
    "os.environ[\"ENDPOINT_ID\"] = ENDPOINT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [3981225844902199296]...\n",
      "......done.\n",
      "Created Vertex AI endpoint: projects/486742359899/locations/us-central1/endpoints/3973489887247597568.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai endpoints create --display-name=${ENDPOINT_NAME} --region=${REGION} --endpoint-id=${ENDPOINT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [5321046734044921856]...\n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai models upload --container-image-uri=${IMAGE_URI} \\\n",
    "--display-name=${MODEL_NAME} \\\n",
    "--model-id=${MODEL_ID}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Waiting for operation [4315618117234458624]...\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................failed.\n",
      "ERROR: (gcloud.ai.endpoints.deploy-model) Model server terminated: model server container terminated: exit_code: \t 2\n",
      "reason: \"Error\"\n",
      "started_at {\n",
      "  seconds: 1681907732\n",
      "}\n",
      "finished_at {\n",
      "  seconds: 1681907743\n",
      "}\n",
      ". Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=486742359899&resource=aiplatform.googleapis.com%252FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%223973489887247597568%22%0Aresource.labels.location%3D%22us-central1%22.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\ngcloud ai endpoints deploy-model ${ENDPOINT_ID} --project=${PROJECT} \\\\\\n    --region=${REGION} \\\\\\n    --model=${MODEL_ID} \\\\\\n    --display-name=${MODEL_NAME}\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_5782/433898265.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\ngcloud ai endpoints deploy-model ${ENDPOINT_ID} --project=${PROJECT} \\\\\\n    --region=${REGION} \\\\\\n    --model=${MODEL_ID} \\\\\\n    --display-name=${MODEL_NAME}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\ngcloud ai endpoints deploy-model ${ENDPOINT_ID} --project=${PROJECT} \\\\\\n    --region=${REGION} \\\\\\n    --model=${MODEL_ID} \\\\\\n    --display-name=${MODEL_NAME}\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "gcloud ai endpoints deploy-model ${ENDPOINT_ID} --project=${PROJECT} \\\n",
    "    --region=${REGION} \\\n",
    "    --model=${MODEL_ID} \\\n",
    "    --display-name=${MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should be deployed within minutes.\n",
    "\n",
    "**Note**: If you are unable to access the generated endpoint link, please ignore it. To see the progress, in your Cloud Console, click on Navigation menu > Vertex AI > Endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2023 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
